{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SETTINGS\n",
    "\n",
    "# Scrapy settings for Pwspider project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = \"Pwspider\"\n",
    "\n",
    "SPIDER_MODULES = [\"Pwspider.spiders\"]\n",
    "NEWSPIDER_MODULE = \"Pwspider.spiders\"\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = \"Pwspider (+http://www.yourdomain.com)\"\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = True\n",
    "\n",
    "DOWNLOAD_HANDLERS = {\n",
    "    \"http\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "    \"https\": \"scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler\",\n",
    "}\n",
    "\n",
    "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
    "\n",
    "PLAYWRIGHT_BROWSER_TYPE = \"firefox\"\n",
    "\n",
    "PLAYWRIGHT_LAUNCH_OPTIONS = {\n",
    "    \"headless\": False,\n",
    "    \"timeout\": 20 * 1000,  # 20 seconds\n",
    "}\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "#    \"Accept-Language\": \"en\",\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    \"Pwspider.middlewares.PwspiderSpiderMiddleware\": 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    \"Pwspider.middlewares.PwspiderDownloaderMiddleware\": 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    \"scrapy.extensions.telnet.TelnetConsole\": None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    \"Pwspider.pipelines.PwspiderPipeline\": 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = \"httpcache\"\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = \"scrapy.extensions.httpcache.FilesystemCacheStorage\"\n",
    "\n",
    "# Set settings whose default value is deprecated to a future-proof value\n",
    "REQUEST_FINGERPRINTER_IMPLEMENTATION = \"2.7\"\n",
    "TWISTED_REACTOR = \"twisted.internet.asyncioreactor.AsyncioSelectorReactor\"\n",
    "FEED_EXPORT_ENCODING = \"utf-8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipelines\n",
    "\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class PwspiderPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MIDDLEWARE\n",
    "\n",
    "# Define here the models for your spider middleware\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "from scrapy import signals\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import is_item, ItemAdapter\n",
    "\n",
    "\n",
    "class PwspiderSpiderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, or item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Request or item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesnâ€™t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n",
    "\n",
    "\n",
    "class PwspiderDownloaderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        # Called for each request that goes through the downloader\n",
    "        # middleware.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this request\n",
    "        # - or return a Response object\n",
    "        # - or return a Request object\n",
    "        # - or raise IgnoreRequest: process_exception() methods of\n",
    "        #   installed downloader middleware will be called\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info(\"Spider opened: %s\" % spider.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ITEMS\n",
    "\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class PwspiderItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy_playwright.page import PageMethod\n",
    "\n",
    "class PwspideySpider(scrapy.Spider):\n",
    "    name = \"pwspidey\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(\n",
    "            'https://www.cfainstitute.org/en/membership/professional-development/refresher-readings#sort=%40refreadingcurriculumyear%20descending',\n",
    "            meta=dict(\n",
    "                playwright=True,\n",
    "                playwright_include_page=True,\n",
    "                playwright_page_methods=[\n",
    "                    PageMethod('wait_for_selector', 'div#coveoD983BA32')\n",
    "                ],\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    async def parse(self, response):\n",
    "        for link in response.css('div.coveo-list-layout.CoveoResult a.CoveoResultLink::attr(href)').getall():\n",
    "            yield scrapy.Request(\n",
    "                response.urljoin(link),\n",
    "                callback=self.parse_details,\n",
    "                meta=dict(\n",
    "                    playwright=True,\n",
    "                    playwright_include_page=True,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "    async def parse_details(self, response):\n",
    "        introduction_texts = response.xpath(\"//h2[contains(@class, 'article-section') and contains(text(), 'Introduction')]/following-sibling::*[self::p or self::br][preceding-sibling::h2[1][contains(text(), 'Introduction')]]//text()\"\n",
    "        ).getall()\n",
    "\n",
    "        learning_outcomes_list = response.xpath(\n",
    "        \"//h2[contains(@class, 'article-section') and contains(text(), 'Learning Outcomes')]/following-sibling::section[1]//ol[1]/li//text() | \" +\n",
    "        \"//h2[contains(@class, 'article-section') and contains(text(), 'Learning Outcomes')]/following-sibling::section[1]//ul[1]/li//text()\"\n",
    "        ).getall()\n",
    "\n",
    "\n",
    "        combined_lo_text = ' '.join([item.strip() for item in learning_outcomes_list if item.strip()])\n",
    "\n",
    "        combined_introduction_text = ' '.join([text.strip() for text in introduction_texts if text.strip()])\n",
    "\n",
    "\n",
    "        yield {\n",
    "            'Title': response.css('h1::text').get(),\n",
    "            'URL': response.url,\n",
    "            'Introduction Summary': combined_introduction_text,\n",
    "            'Learning Outcomes': combined_lo_text,\n",
    "            #'Level': reponse.css('div.content-utility.content-utility-curriculum::text')\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "async def parse(self, response):\n",
    "\tfor link in response.css('div.coveo-result-cell'):\n",
    "\t    yield{\n",
    "\t\t\t    'title': link.css('h4::text').get(),\n",
    "\t\t\t\t#'link': link.xpath('@href').get()\n",
    "\t\t\t}\n",
    "'''            \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
